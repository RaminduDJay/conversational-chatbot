{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Import Dependencies\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. Define Constants and Patterns\n",
    "\n",
    "# Configure constants\n",
    "DATA_DIR = \"../data\"\n",
    "DEFAULT_ENCODING = \"utf-8\"\n",
    "MAX_TOKENS = 500\n",
    "MIN_TOKENS = 300\n",
    "TOKENIZER_MODEL = \"cl100k_base\"\n",
    "\n",
    "# Compile regex patterns once for efficiency\n",
    "TITLE_PATTERN = re.compile(r\"Title: (.+)\")\n",
    "URL_PATTERN = re.compile(r\"URL Source: (.+)\")\n",
    "CONTENT_PATTERN = re.compile(r\"Markdown Content:(.+)\", re.DOTALL)\n",
    "SUBTOPIC_PATTERN = re.compile(r\"^(.*)\\n-+\\n\", re.MULTILINE)\n",
    "DIALOGUE_PATTERN = re.compile(r\"(?P<speaker>\\w+)\\s\\[\\((?P<timestamp>\\d{2}:\\d{2}:\\d{2})\\)\\]\\((?P<url>https:\\/\\/youtube\\.com\\/watch\\?v=[^&]+&t=\\d+)\\)\\s(?P<text>.+)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. File Loading Functions\n",
    "\n",
    "def load_transcripts(data_dir=DATA_DIR):\n",
    "    \"\"\"\n",
    "    Load all transcript files from the data directory.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing transcript files\n",
    "        \n",
    "    Returns:\n",
    "        A list of transcript strings\n",
    "    \"\"\"\n",
    "    transcripts = []\n",
    "    try:\n",
    "        for file_name in os.listdir(data_dir):\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                try:\n",
    "                    with open(os.path.join(data_dir, file_name), 'r', encoding=DEFAULT_ENCODING) as f:\n",
    "                        transcripts.append(f.read())\n",
    "                    print(f\"Loaded: {file_name}\")\n",
    "                except (IOError, UnicodeDecodeError) as e:\n",
    "                    print(f\"Error reading file {file_name}: {e}\")\n",
    "        return transcripts\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Data directory not found: {data_dir}\")\n",
    "        return []\n",
    "\n",
    "# Let's load our transcripts\n",
    "transcripts = load_transcripts()\n",
    "print(f\"Loaded {len(transcripts)} transcript files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Basic Transcript Parsing\n",
    "\n",
    "def parse_transcript(transcript):\n",
    "    \"\"\"\n",
    "    Parse a transcript to extract title, URL, and content.\n",
    "    \n",
    "    Args:\n",
    "        transcript: Raw transcript text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with title, url, and content fields\n",
    "    \"\"\"\n",
    "    title_match = TITLE_PATTERN.search(transcript)\n",
    "    url_match = URL_PATTERN.search(transcript)\n",
    "    content_match = CONTENT_PATTERN.search(transcript)\n",
    "\n",
    "    result = {\n",
    "        \"title\": title_match.group(1) if title_match else None,\n",
    "        \"url\": url_match.group(1) if url_match else None,\n",
    "        \"content\": content_match.group(1).strip() if content_match else None\n",
    "    }\n",
    "    \n",
    "    # Check if all fields were successfully extracted\n",
    "    missing_fields = [field for field, value in result.items() if value is None]\n",
    "    if missing_fields:\n",
    "        print(f\"Warning: Missing fields in transcript: {', '.join(missing_fields)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the parse_transcript function with sample data\n",
    "if transcripts:\n",
    "    sample_transcript = transcripts[0]\n",
    "    parsed_sample = parse_transcript(sample_transcript)\n",
    "    print(\"\\nSample Transcript Info:\")\n",
    "    print(f\"Title: {parsed_sample['title']}\")\n",
    "    print(f\"URL: {parsed_sample['url']}\")\n",
    "    print(f\"Content Length: {len(parsed_sample['content']) if parsed_sample['content'] else 0} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. Tokenization Utilities\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(TOKENIZER_MODEL)\n",
    "\n",
    "def get_token_count(text):\n",
    "    \"\"\"\n",
    "    Count tokens in a string.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to count tokens in\n",
    "        \n",
    "    Returns:\n",
    "        Number of tokens\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def chunk_text(text, max_tokens=MAX_TOKENS, min_tokens=MIN_TOKENS):\n",
    "    \"\"\"\n",
    "    Chunk text based on token count, preserving sentence structure where possible.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        max_tokens: Maximum tokens per chunk\n",
    "        min_tokens: Minimum tokens per chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    # Try to find natural break points like end of sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    current_sentence_tokens = []\n",
    "    current_chunk_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.encode(sentence)\n",
    "        \n",
    "        # If a single sentence exceeds max_tokens, we'll need to break it up\n",
    "        if len(sentence_tokens) > max_tokens:\n",
    "            # Process the oversized sentence word by word\n",
    "            words = sentence.split()\n",
    "            current_word_tokens = []\n",
    "            \n",
    "            for word in words:\n",
    "                word_tokens = tokenizer.encode(word + \" \")\n",
    "                if len(current_chunk_tokens) + len(word_tokens) <= max_tokens:\n",
    "                    current_chunk_tokens.extend(word_tokens)\n",
    "                    current_word_tokens.extend(word_tokens)\n",
    "                else:\n",
    "                    # Save current chunk and start a new one\n",
    "                    chunks.append(tokenizer.decode(current_chunk_tokens))\n",
    "                    current_chunk_tokens = word_tokens\n",
    "                    current_word_tokens = word_tokens\n",
    "            \n",
    "            # Add any remaining tokens from the oversized sentence\n",
    "            if current_word_tokens:\n",
    "                if len(current_chunk_tokens) < min_tokens and chunks:\n",
    "                    # Combine with previous chunk if this one is too small\n",
    "                    previous_chunk = chunks.pop()\n",
    "                    previous_tokens = tokenizer.encode(previous_chunk)\n",
    "                    combined = previous_tokens + current_chunk_tokens\n",
    "                    chunks.append(tokenizer.decode(combined))\n",
    "                else:\n",
    "                    chunks.append(tokenizer.decode(current_chunk_tokens))\n",
    "                current_chunk_tokens = []\n",
    "                \n",
    "        # Normal case: sentence fits within max_tokens\n",
    "        elif len(current_chunk_tokens) + len(sentence_tokens) <= max_tokens:\n",
    "            current_chunk_tokens.extend(sentence_tokens)\n",
    "        else:\n",
    "            # Save current chunk and start new one with this sentence\n",
    "            chunks.append(tokenizer.decode(current_chunk_tokens))\n",
    "            current_chunk_tokens = sentence_tokens\n",
    "    \n",
    "    # Handle any remaining content\n",
    "    if current_chunk_tokens:\n",
    "        if len(current_chunk_tokens) < min_tokens and chunks:\n",
    "            # Combine with previous chunk if this one is too small\n",
    "            previous_chunk = chunks.pop()\n",
    "            previous_tokens = tokenizer.encode(previous_chunk)\n",
    "            combined = previous_tokens + current_chunk_tokens\n",
    "            chunks.append(tokenizer.decode(combined))\n",
    "        else:\n",
    "            chunks.append(tokenizer.decode(current_chunk_tokens))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test the tokenizer on a small sample\n",
    "if parsed_sample and parsed_sample['content']:\n",
    "    sample_text = parsed_sample['content'][:1000]  # First 1000 chars\n",
    "    token_count = get_token_count(sample_text)\n",
    "    print(f\"\\nSample text token count: {token_count}\")\n",
    "    \n",
    "    # Test chunking\n",
    "    chunks = chunk_text(sample_text, max_tokens=200, min_tokens=100)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {get_token_count(chunk)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Dialogue Extraction Functions\n",
    "\n",
    "def extract_dialogues(content_block):\n",
    "    \"\"\"\n",
    "    Extract dialogues from a content block.\n",
    "    \n",
    "    Args:\n",
    "        content_block: A string containing dialogue text\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (dialogues, speakers, timestamp)\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "    speakers = []\n",
    "    timestamp = None\n",
    "    \n",
    "    matches = DIALOGUE_PATTERN.findall(content_block)\n",
    "    \n",
    "    for match in matches:\n",
    "        speaker, time, url, text = match\n",
    "        if timestamp is None:\n",
    "            timestamp = f\"[({time})]({url})\"\n",
    "        \n",
    "        if speaker not in speakers:\n",
    "            speakers.append(speaker)\n",
    "            \n",
    "        dialogues.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"text\": text,\n",
    "            \"timestamp\": time,\n",
    "            \"url\": url\n",
    "        })\n",
    "        \n",
    "    return dialogues, speakers, timestamp\n",
    "\n",
    "def format_dialogues(dialogues):\n",
    "    \"\"\"\n",
    "    Format a list of dialogue dictionaries into strings.\n",
    "    \n",
    "    Args:\n",
    "        dialogues: List of dialogue dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        List of formatted dialogue strings\n",
    "    \"\"\"\n",
    "    return [f\"{d['speaker']}: {d['text']} \\n\" for d in dialogues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Subtopic Processing\n",
    "\n",
    "def parse_transcript_by_subtopic(data):\n",
    "    \"\"\"\n",
    "    Parse transcript content into subtopic chunks.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary with transcript data\n",
    "        \n",
    "    Returns:\n",
    "        List of subtopic chunks\n",
    "    \"\"\"\n",
    "    transcript = data.get(\"content\")\n",
    "    if not transcript:\n",
    "        print(\"Warning: No content found in transcript data\")\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    subtopics = SUBTOPIC_PATTERN.split(transcript)\n",
    "    \n",
    "    subtopic_count = (len(subtopics) - 1) // 2\n",
    "    print(f\"Found {subtopic_count} subtopics in transcript\")\n",
    "    \n",
    "    for i in range(1, len(subtopics), 2):\n",
    "        subtopic = subtopics[i].strip()\n",
    "        content_block = subtopics[i + 1] if i + 1 < len(subtopics) else \"\"\n",
    "        \n",
    "        dialogues, speakers, timestamp = extract_dialogues(content_block)\n",
    "        formatted_text = format_dialogues(dialogues)\n",
    "        \n",
    "        # Skip empty subtopics\n",
    "        if not formatted_text:\n",
    "            print(f\"Warning: Empty content in subtopic '{subtopic}'\")\n",
    "            continue\n",
    "        \n",
    "        chunks.append({\n",
    "            \"subtopic\": subtopic,\n",
    "            \"content\": formatted_text,\n",
    "            \"metadata\": {\n",
    "                \"speakers\": speakers,\n",
    "                \"dialogue_count\": len(dialogues),\n",
    "                \"title\": data.get(\"title\"),\n",
    "                \"url\": data.get(\"url\"),\n",
    "                \"timestamp\": timestamp\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# Let's test parsing by subtopic\n",
    "if parsed_sample and parsed_sample['content']:\n",
    "    subtopic_chunks = parse_transcript_by_subtopic(parsed_sample)\n",
    "    print(f\"\\nExtracted {len(subtopic_chunks)} subtopic chunks\")\n",
    "    \n",
    "    # Display the first few subtopics\n",
    "    for i, chunk in enumerate(subtopic_chunks[:3]):  # Show first 3\n",
    "        print(f\"\\nSubtopic {i+1}: {chunk['subtopic']}\")\n",
    "        print(f\"Speakers: {', '.join(chunk['metadata']['speakers'])}\")\n",
    "        print(f\"Dialogue count: {chunk['metadata']['dialogue_count']}\")\n",
    "        # Print a sample of the content (first 2 dialogues)\n",
    "        if chunk['content']:\n",
    "            print(\"Sample content:\")\n",
    "            for dialogue in chunk['content'][:2]:\n",
    "                print(f\"  {dialogue.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 8. Token-Based Chunking\n",
    "\n",
    "def parse_and_chunk_transcript_by_subtopic(data, max_tokens=MAX_TOKENS, min_tokens=MIN_TOKENS):\n",
    "    \"\"\"\n",
    "    Parse transcript and create token-sized chunks within subtopics.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary with transcript data\n",
    "        max_tokens: Maximum tokens per chunk\n",
    "        min_tokens: Minimum tokens per chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of chunked subtopics\n",
    "    \"\"\"\n",
    "    transcript = data.get(\"content\")\n",
    "    if not transcript:\n",
    "        print(\"Warning: No content found in transcript data\")\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    subtopics = SUBTOPIC_PATTERN.split(transcript)\n",
    "    \n",
    "    subtopic_count = (len(subtopics) - 1) // 2\n",
    "    print(f\"Processing {subtopic_count} subtopics with token limits ({min_tokens}-{max_tokens})\")\n",
    "    \n",
    "    total_dialogues = 0\n",
    "    chunked_subtopics = 0\n",
    "    \n",
    "    for i in range(1, len(subtopics), 2):\n",
    "        subtopic = subtopics[i].strip()\n",
    "        content_block = subtopics[i + 1] if i + 1 < len(subtopics) else \"\"\n",
    "        \n",
    "        dialogues, speakers, timestamp = extract_dialogues(content_block)\n",
    "        formatted_text = format_dialogues(dialogues)\n",
    "        \n",
    "        total_dialogues += len(dialogues)\n",
    "        \n",
    "        # Skip empty subtopics\n",
    "        if not formatted_text:\n",
    "            continue\n",
    "        \n",
    "        # Check if we need to chunk based on token count\n",
    "        combined_text = ''.join(formatted_text)\n",
    "        token_count = get_token_count(combined_text)\n",
    "        \n",
    "        if token_count > max_tokens:\n",
    "            chunked_subtopics += 1\n",
    "            # Use sentence-aware chunking to preserve dialogue context\n",
    "            token_chunks = chunk_text(combined_text, max_tokens, min_tokens)\n",
    "            for chunk_idx, chunk in enumerate(token_chunks):\n",
    "                chunks.append({\n",
    "                    \"subtopic\": f\"{subtopic} (part {chunk_idx+1}/{len(token_chunks)})\",\n",
    "                    \"content\": chunk,  # A single string\n",
    "                    \"metadata\": {\n",
    "                        \"speakers\": speakers,\n",
    "                        \"token_count\": get_token_count(chunk),\n",
    "                        \"title\": data.get(\"title\"),\n",
    "                        \"url\": data.get(\"url\"),\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"is_chunked\": True,\n",
    "                        \"original_subtopic\": subtopic\n",
    "                    }\n",
    "                })\n",
    "        else:\n",
    "            chunks.append({\n",
    "                \"subtopic\": subtopic,\n",
    "                \"content\": formatted_text,  # A list of strings\n",
    "                \"metadata\": {\n",
    "                    \"speakers\": speakers,\n",
    "                    \"dialogue_count\": len(dialogues),\n",
    "                    \"token_count\": token_count,\n",
    "                    \"title\": data.get(\"title\"),\n",
    "                    \"url\": data.get(\"url\"),\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"is_chunked\": False\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"Processed {total_dialogues} dialogues, chunked {chunked_subtopics} subtopics\")\n",
    "    return chunks\n",
    "\n",
    "# Test parsing and chunking with a sample\n",
    "if parsed_sample and parsed_sample['content']:\n",
    "    chunked_results = parse_and_chunk_transcript_by_subtopic(parsed_sample)\n",
    "    print(f\"\\nCreated {len(chunked_results)} chunks after token-based chunking\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_tokens = sum(chunk['metadata'].get('token_count', 0) for chunk in chunked_results)\n",
    "    avg_tokens = total_tokens / len(chunked_results) if chunked_results else 0\n",
    "    \n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    print(f\"Average tokens per chunk: {avg_tokens:.1f}\")\n",
    "    \n",
    "    # Count chunked vs non-chunked subtopics\n",
    "    chunked = sum(1 for chunk in chunked_results if chunk['metadata'].get('is_chunked', False))\n",
    "    non_chunked = len(chunked_results) - chunked\n",
    "    print(f\"Chunked subtopics: {chunked}\")\n",
    "    print(f\"Non-chunked subtopics: {non_chunked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 9. Token Count Analysis\n",
    "\n",
    "def get_token_counts_by_subtopic(subtopics):\n",
    "    \"\"\"\n",
    "    Get token counts for each subtopic.\n",
    "    \n",
    "    Args:\n",
    "        subtopics: List of subtopic dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with subtopic names and token counts\n",
    "    \"\"\"\n",
    "    token_counts = []\n",
    "    for subtopic in subtopics:\n",
    "        content = subtopic['content']\n",
    "        \n",
    "        # Handle both string and list content formats\n",
    "        if isinstance(content, list):\n",
    "            content_text = ''.join(content)\n",
    "        else:\n",
    "            content_text = content\n",
    "            \n",
    "        token_count = get_token_count(content_text)\n",
    "        \n",
    "        token_counts.append({\n",
    "            'subtopic': subtopic['subtopic'],\n",
    "            'token_count': token_count\n",
    "        })\n",
    "        \n",
    "    return token_counts\n",
    "\n",
    "# Analyze token counts for our sample\n",
    "if 'chunked_results' in locals() and chunked_results:\n",
    "    token_counts = get_token_counts_by_subtopic(chunked_results)\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_tokens = pd.DataFrame(token_counts)\n",
    "    \n",
    "    print(\"\\nToken count statistics:\")\n",
    "    print(f\"Min: {df_tokens['token_count'].min()}\")\n",
    "    print(f\"Max: {df_tokens['token_count'].max()}\")\n",
    "    print(f\"Mean: {df_tokens['token_count'].mean():.1f}\")\n",
    "    print(f\"Median: {df_tokens['token_count'].median()}\")\n",
    "    \n",
    "    # Plot token distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df_tokens['token_count'], bins=10, alpha=0.7)\n",
    "    plt.axvline(x=MAX_TOKENS, color='r', linestyle='--', label=f'Max tokens limit ({MAX_TOKENS})')\n",
    "    plt.xlabel('Token Count')\n",
    "    plt.ylabel('Number of Chunks')\n",
    "    plt.title('Token Count Distribution Across Chunks')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 10. Process All Transcripts\n",
    "\n",
    "def process_all_transcripts(transcript_list, chunk_by_tokens=True):\n",
    "    \"\"\"\n",
    "    Process all transcripts in the provided list.\n",
    "    \n",
    "    Args:\n",
    "        transcript_list: List of transcript strings\n",
    "        chunk_by_tokens: Whether to chunk by token count\n",
    "        \n",
    "    Returns:\n",
    "        List of processed transcript chunks\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for i, transcript in enumerate(transcript_list):\n",
    "        try:\n",
    "            print(f\"\\nProcessing transcript {i+1}/{len(transcript_list)}\")\n",
    "            parsed = parse_transcript(transcript)\n",
    "            \n",
    "            if chunk_by_tokens:\n",
    "                chunks = parse_and_chunk_transcript_by_subtopic(parsed)\n",
    "            else:\n",
    "                chunks = parse_transcript_by_subtopic(parsed)\n",
    "                \n",
    "            all_chunks.extend(chunks)\n",
    "            print(f\"Added {len(chunks)} chunks\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing transcript {i+1}: {e}\")\n",
    "                \n",
    "    return all_chunks\n",
    "\n",
    "# Process all our transcripts\n",
    "all_processed_chunks = process_all_transcripts(transcripts)\n",
    "\n",
    "print(f\"\\nTotal processed chunks across all transcripts: {len(all_processed_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 11. Analyzing Speaker Distributions\n",
    "\n",
    "if all_processed_chunks:\n",
    "    # Collect all speakers\n",
    "    all_speakers = set()\n",
    "    for chunk in all_processed_chunks:\n",
    "        all_speakers.update(chunk['metadata']['speakers'])\n",
    "    \n",
    "    print(f\"\\nTotal unique speakers across all transcripts: {len(all_speakers)}\")\n",
    "    print(f\"Speakers: {', '.join(sorted(all_speakers))}\")\n",
    "    \n",
    "    # Count speaker occurrences\n",
    "    speaker_count = {}\n",
    "    for chunk in all_processed_chunks:\n",
    "        for speaker in chunk['metadata']['speakers']:\n",
    "            speaker_count[speaker] = speaker_count.get(speaker, 0) + 1\n",
    "    \n",
    "    # Plot speaker distribution\n",
    "    speakers_df = pd.DataFrame(list(speaker_count.items()), columns=['Speaker', 'Chunk Count'])\n",
    "    speakers_df = speakers_df.sort_values('Chunk Count', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(speakers_df['Speaker'], speakers_df['Chunk Count'], color='skyblue')\n",
    "    plt.xlabel('Speaker')\n",
    "    plt.ylabel('Number of Chunks Appearing In')\n",
    "    plt.title('Speaker Participation Across All Transcripts')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 12. Subtopic Analysis\n",
    "\n",
    "if all_processed_chunks:\n",
    "    # Analyze subtopics\n",
    "    subtopics = [chunk['subtopic'] for chunk in all_processed_chunks]\n",
    "    subtopic_counts = pd.Series(subtopics).value_counts()\n",
    "    \n",
    "    print(f\"\\nTotal unique subtopics: {len(subtopic_counts)}\")\n",
    "    \n",
    "    # Show top subtopics\n",
    "    print(\"\\nTop 10 subtopics by frequency:\")\n",
    "    print(subtopic_counts.head(10))\n",
    "    \n",
    "    # Plot subtopic distribution (top 15)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    subtopic_counts.head(15).plot(kind='barh', color='lightgreen')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Subtopic')\n",
    "    plt.title('Most Common Subtopics')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 13. Save Processed Data\n",
    "\n",
    "import json\n",
    "\n",
    "def save_processed_data(chunks, output_file=\"processed_transcripts.json\"):\n",
    "    \"\"\"\n",
    "    Save processed chunks to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of processed transcript chunks\n",
    "        output_file: Filename to save to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nSaved {len(chunks)} processed chunks to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data: {e}\")\n",
    "\n",
    "# Save our processed data\n",
    "if all_processed_chunks:\n",
    "    save_processed_data(all_processed_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 14. Summary and Next Steps\n",
    "\n",
    "print(\"\\n## Summary ##\")\n",
    "print(f\"- Processed {len(transcripts)} transcript files\")\n",
    "print(f\"- Created {len(all_processed_chunks)} content chunks\")\n",
    "print(f\"- Identified {len(all_speakers)} unique speakers\")\n",
    "\n",
    "print(\"\\n## Next Steps ##\")\n",
    "print(\"1. Use the processed chunks for further analysis\")\n",
    "print(\"2. Consider implementing topic modeling to identify key themes\")\n",
    "print(\"3. Perform sentiment analysis on the dialogue\")\n",
    "print(\"4. Create speaker interaction networks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
