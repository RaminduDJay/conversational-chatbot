{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and initialize clients\n",
    "import logging\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "from zep_cloud.client import Zep\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging for notebook environment\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check required environment variables\n",
    "required_vars = [\"QDRANT_CLOUD_API_KEY\", \"OPENAI_API_KEY\", \"ZEP_API_KEY\"]\n",
    "for var in required_vars:\n",
    "    if not os.getenv(var):\n",
    "        logger.warning(f\"Missing environment variable: {var}\")\n",
    "\n",
    "# Initialize clients\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://3973cdf9-4ba6-40b1-ae92-b2f952f82fb9.europe-west3-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=os.getenv(\"QDRANT_CLOUD_API_KEY\"),\n",
    ")\n",
    "\n",
    "zep_client = Zep(\n",
    "    api_key=os.environ.get('ZEP_API_KEY'),\n",
    ")\n",
    "\n",
    "openai_client = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Zep user (keep for potential future use)\n",
    "# Create user if user not created\n",
    "try:\n",
    "    zep_client.user.add(\n",
    "        email=\"test@email.com\",\n",
    "        first_name=\"Test\",\n",
    "        last_name=\"User\",\n",
    "        user_id=\"user_1\",  # do not change the id\n",
    "    )\n",
    "    logger.info(\"Zep user created or already exists\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating Zep user: {e}\")\n",
    "\n",
    "# Check existing collections\n",
    "collections = qdrant_client.get_collections()\n",
    "logger.info(f\"Existing collections: {[c.name for c in collections.collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transcript loading and parsing functions\n",
    "def load_transcripts(data_dir):\n",
    "    \"\"\"Load transcript text files from a directory.\"\"\"\n",
    "    transcripts = []\n",
    "    file_count = 0\n",
    "    \n",
    "    for file_name in os.listdir(data_dir):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as f:\n",
    "                transcripts.append(f.read())\n",
    "                file_count += 1\n",
    "    \n",
    "    logger.info(f\"Loaded {file_count} transcript files from {data_dir}\")\n",
    "    return transcripts\n",
    "\n",
    "\n",
    "def parse_transcript(transcript):\n",
    "    \"\"\"Parse transcript text to extract metadata.\"\"\"\n",
    "    title_match = re.search(r\"Title: (.+)\", transcript)\n",
    "    url_match = re.search(r\"URL Source: (.+)\", transcript)\n",
    "    content_match = re.search(r\"Markdown Content:(.+)\", transcript, re.DOTALL)\n",
    "\n",
    "    parsed_data = {\n",
    "        \"title\": title_match.group(1) if title_match else \"Unknown Title\",\n",
    "        \"url\": url_match.group(1) if url_match else \"No URL\",\n",
    "        \"content\": content_match.group(1).strip() if content_match else \"\"\n",
    "    }\n",
    "    \n",
    "    # Simple validation\n",
    "    if not parsed_data[\"content\"]:\n",
    "        logger.warning(f\"No content found for transcript: {parsed_data['title']}\")\n",
    "        \n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization and chunking functions\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def get_token_count_by_subtopic(subtopics):\n",
    "    \"\"\"Get token counts for each subtopic for analysis.\"\"\"\n",
    "    token_counts = []\n",
    "    for subtopic in subtopics:\n",
    "        if isinstance(subtopic['content'], list):\n",
    "            content = ' '.join(subtopic['content'])\n",
    "        else:\n",
    "            content = subtopic['content']\n",
    "            \n",
    "        tokens = tokenizer.encode(content)\n",
    "        token_counts.append({\n",
    "            'subtopic': subtopic['subtopic'],\n",
    "            'token_count': len(tokens)\n",
    "        })\n",
    "    return token_counts\n",
    "\n",
    "\n",
    "def chunk_text(text, max_tokens=500, min_tokens=300):\n",
    "    \"\"\"Split text into chunks based on token count.\"\"\"\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        current_chunk.append(token)\n",
    "        # If the current chunk exceeds the max token limit\n",
    "        if len(current_chunk) >= max_tokens:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            \n",
    "    # Handle the last chunk, ensure it meets the minimum size requirement\n",
    "    if current_chunk:\n",
    "        if len(current_chunk) < min_tokens and chunks:\n",
    "            # If the last chunk is smaller than the minimum, merge it with the previous chunk\n",
    "            chunks[-1].extend(current_chunk)\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            \n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transcript parsing and chunking by subtopic\n",
    "def parse_and_chunk_transcript_by_subtopic(data):\n",
    "    \"\"\"Parse transcript content and split it by subtopics and token count.\"\"\"\n",
    "    if not data or not data.get(\"content\"):\n",
    "        logger.warning(\"Invalid transcript data\")\n",
    "        return []\n",
    "        \n",
    "    transcript = data[\"content\"]\n",
    "    # Regex to find subtopics (e.g., Introduction, Education)\n",
    "    subtopic_pattern = re.compile(r\"^(.*)\\n-+\\n\", re.MULTILINE)\n",
    "    # Regex to capture speaker dialogue (e.g., Destiny [(00:00:00)]...)\n",
    "    dialogue_pattern = re.compile(r\"(?P<speaker>\\w+)\\s\\[\\((?P<timestamp>\\d{2}:\\d{2}:\\d{2})\\)\\]\\((?P<url>https:\\/\\/youtube\\.com\\/watch\\?v=[^&]+&t=\\d+)\\)\\s(?P<text>.+)\")\n",
    "    \n",
    "    chunks = []\n",
    "    subtopics = subtopic_pattern.split(transcript)\n",
    "\n",
    "    for i in range(1, len(subtopics), 2):\n",
    "        try:\n",
    "            subtopic = subtopics[i].strip()\n",
    "            content_block = subtopics[i + 1] if i + 1 < len(subtopics) else \"\"\n",
    "            \n",
    "            # Find all dialogues within this subtopic\n",
    "            dialogues = dialogue_pattern.findall(content_block)\n",
    "            \n",
    "            formatted_text = []\n",
    "            speakers = []\n",
    "            tstamp = None\n",
    "            \n",
    "            for dialogue in dialogues:\n",
    "                speaker, timestamp, url, text = dialogue\n",
    "                if tstamp is None:\n",
    "                    tstamp = f\"[({timestamp})]({url})\"\n",
    "                \n",
    "                if speaker not in speakers:\n",
    "                    speakers.append(speaker)\n",
    "                \n",
    "                formatted_text.append(f\"{speaker}: {text} \\n\")\n",
    "            \n",
    "            # Join the formatted text and check token count\n",
    "            joined_text = ' '.join(formatted_text)\n",
    "            tokens_enc = tokenizer.encode(joined_text)\n",
    "            tok_count = len(tokens_enc)\n",
    "            \n",
    "            if tok_count > 500:\n",
    "                token_chunks = chunk_text(joined_text)\n",
    "                for chunk in token_chunks:\n",
    "                    current_chunk = {\n",
    "                        \"subtopic\": subtopic,\n",
    "                        \"content\": chunk,\n",
    "                        \"metadata\": {\n",
    "                            \"speakers\": speakers,\n",
    "                            \"dialogue_count\": len(dialogues),\n",
    "                            \"title\": data[\"title\"],\n",
    "                            \"url\": data[\"url\"],\n",
    "                            \"timestamp\": tstamp\n",
    "                        }\n",
    "                    }\n",
    "                    chunks.append(current_chunk)\n",
    "            else:\n",
    "                current_chunk = {\n",
    "                    \"subtopic\": subtopic,\n",
    "                    \"content\": joined_text,\n",
    "                    \"metadata\": {\n",
    "                        \"speakers\": speakers,\n",
    "                        \"dialogue_count\": len(formatted_text),\n",
    "                        \"title\": data[\"title\"],\n",
    "                        \"url\": data[\"url\"],\n",
    "                        \"timestamp\": tstamp\n",
    "                    }\n",
    "                }\n",
    "                chunks.append(current_chunk)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing subtopic {subtopic}: {e}\")\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding and vector database functions\n",
    "def get_embedding(text):\n",
    "    \"\"\"Get OpenAI embedding for a single text.\"\"\"\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_embeddings_batch(texts, batch_size=20):\n",
    "    \"\"\"Get OpenAI embeddings for multiple texts efficiently.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process in batches to avoid rate limits\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            response = openai_client.embeddings.create(\n",
    "                input=batch, \n",
    "                model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            logger.info(f\"Processed embeddings batch {i//batch_size + 1}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "            # Fill with None for failed embeddings to maintain alignment\n",
    "            all_embeddings.extend([None] * len(batch))\n",
    "    \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding function (keep for interactive exploration)\n",
    "test_embed = get_embedding(\"hello world\")\n",
    "print(f\"Embedding vector size: {len(test_embed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collection creation function\n",
    "def create_collections(collection_name, vector_size=1536):\n",
    "    \"\"\"Create new collection in Qdrant cloud if it doesn't exist.\"\"\"\n",
    "    # Check if collection exists\n",
    "    collections = qdrant_client.get_collections()\n",
    "    collection_names = [c.name for c in collections.collections]\n",
    "    \n",
    "    if collection_name in collection_names:\n",
    "        logger.info(f\"Collection {collection_name} already exists\")\n",
    "        return\n",
    "        \n",
    "    # Create collection\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=vector_size, \n",
    "            distance=models.Distance.COSINE,\n",
    "            hnsw_config=models.HnswConfigDiff(\n",
    "                m=16,\n",
    "                ef_construct=100,\n",
    "                full_scan_threshold=10000,\n",
    "                max_indexing_threads=0\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create indexes on metadata fields and full text\n",
    "    index_fields = [\n",
    "        (\"subtopic\", models.PayloadSchemaType.KEYWORD),\n",
    "        (\"speakers\", models.PayloadSchemaType.KEYWORD),\n",
    "        (\"title\", models.PayloadSchemaType.KEYWORD),\n",
    "        (\"content\", models.PayloadSchemaType.TEXT)\n",
    "    ]\n",
    "    \n",
    "    for field_name, field_schema in index_fields:\n",
    "        qdrant_client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=field_name,\n",
    "            field_schema=field_schema\n",
    "        )\n",
    "        \n",
    "    logger.info(f\"Created collection {collection_name} with indexes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch upsert function for Qdrant\n",
    "def batch_upsert_to_qdrant(collection_name, chunks, batch_size=100):\n",
    "    \"\"\"Process and upload chunks to Qdrant in batches.\"\"\"\n",
    "    total_chunks = len(chunks)\n",
    "    successful_uploads = 0\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        batch_texts = [chunk[\"content\"] for chunk in batch]\n",
    "        \n",
    "        # Get embeddings for the batch\n",
    "        embeddings = get_embeddings_batch(batch_texts)\n",
    "        \n",
    "        # Create points for successful embeddings\n",
    "        points = []\n",
    "        for chunk, embedding in zip(batch, embeddings):\n",
    "            if embedding is None:\n",
    "                continue\n",
    "                \n",
    "            points.append(models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"subtopic\": chunk[\"subtopic\"],\n",
    "                    \"speakers\": chunk[\"metadata\"][\"speakers\"],\n",
    "                    \"content\": chunk[\"content\"],\n",
    "                    \"title\": chunk[\"metadata\"][\"title\"],\n",
    "                    \"url\": chunk[\"metadata\"][\"url\"],\n",
    "                    \"timestamp\": chunk[\"metadata\"][\"timestamp\"]\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        # Upsert points if any were created\n",
    "        if points:\n",
    "            try:\n",
    "                qdrant_client.upsert(\n",
    "                    collection_name=collection_name,\n",
    "                    points=points\n",
    "                )\n",
    "                successful_uploads += len(points)\n",
    "                logger.info(f\"Uploaded {len(points)} points (batch {i//batch_size + 1})\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Upload complete: {successful_uploads}/{total_chunks} chunks successfully uploaded\")\n",
    "    return successful_uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing - Load transcripts and prepare chunks\n",
    "# Define the collection name\n",
    "COLLECTION_NAME = \"podcasts\"\n",
    "VECTOR_SIZE = 1536  # Size of OpenAI's text-embedding-3-small model output\n",
    "\n",
    "# Create the collection\n",
    "create_collections(COLLECTION_NAME, VECTOR_SIZE)\n",
    "\n",
    "# Load and process transcripts\n",
    "data_dir = \"../data\"\n",
    "transcripts = load_transcripts(data_dir)\n",
    "logger.info(f\"Processing {len(transcripts)} transcripts\")\n",
    "\n",
    "# Prepare all chunks before embedding\n",
    "all_chunks = []\n",
    "for idx, transcript in enumerate(transcripts):\n",
    "    data = parse_transcript(transcript)\n",
    "    if not data[\"content\"]:\n",
    "        logger.warning(f\"Skipping transcript #{idx} due to empty content\")\n",
    "        continue\n",
    "        \n",
    "    chunks = parse_and_chunk_transcript_by_subtopic(data)\n",
    "    all_chunks.extend(chunks)\n",
    "    logger.info(f\"Transcript #{idx} produced {len(chunks)} chunks\")\n",
    "\n",
    "logger.info(f\"Total chunks to process: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload chunks to Qdrant in batches\n",
    "# Set batch size for processing\n",
    "BATCH_SIZE = 50\n",
    "successful_uploads = batch_upsert_to_qdrant(COLLECTION_NAME, all_chunks, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final count and verification\n",
    "# Count how many chunks we processed in total\n",
    "total_chunk_count = len(all_chunks)\n",
    "\n",
    "# Get count of documents in the collection\n",
    "try:\n",
    "    collection_info = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    points_count = collection_info.points_count\n",
    "    logger.info(f\"Collection contains {points_count} points\")\n",
    "    logger.info(f\"Upload efficiency: {successful_uploads}/{total_chunk_count} chunks ({successful_uploads/total_chunk_count:.1%})\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error getting collection info: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
